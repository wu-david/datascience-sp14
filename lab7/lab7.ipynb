{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# k-Nearest Neighbor Methods and Image Data\n",
      "\n",
      "In this lab we will explore MNIST, a classic machine learning data set of images of handwritten digits (i.e., 0, 1, 2, 3, ...).\n",
      "In addition, we will investigate an intuitive, yet powerful learning method called k-nearest neighbors (KNN).\n",
      "Even though the type of data is different from what we've worked with so far, we'll see how to apply familiar tools to the data, namely, scikit learn and matplotlib for machine learning and plotting in python.\n",
      "\n",
      "Don't forget to fill out the [response form](https://docs.google.com/a/berkeley.edu/forms/d/188vgp28CXJrJ_qRpyGGBNwG0RjPvAfzirH9zLeSTMKo/viewform).\n",
      "\n",
      "And if you haven't already, fetch the data by running to following code (it will download into the `DATA_PATH` directory):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import pylab\n",
      "from sklearn.datasets import fetch_mldata\n",
      "DATA_PATH = '~/datascience-sp14/lab7/mldata'\n",
      "mnist = fetch_mldata('MNIST original', data_home=DATA_PATH)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## MNIST\n",
      "\n",
      "<center>\n",
      "    <img src=\"http://3.bp.blogspot.com/_UpN7DfJA0j4/TJtUBWPk0SI/AAAAAAAAABY/oWPMtmqJn3k/s320/mnist_originals.png\">\n",
      "</center>\n",
      "\n",
      "The [MNIST database](http://yann.lecun.com/exdb/mnist/) of handwritten digits is a collection of labeled images that has been used to evaluate machine learning techniques since the '90's.\n",
      "The core application of the MNIST data is to train computer vision systems to recognize handwritten text.\n",
      "The post office, for example, is a major user of such systems---addresses on letters and packages are all photographed, read, and routed digitally, with only a few ambiguous cases verified by a human.\n",
      "\n",
      "The MNIST data set has also become a reliable benchmark for learning methods.\n",
      "It's small, but not tiny, and the data dimensionality (28x28 pixels) is big enough to cause some \"curse of dimensionality\" issues.\n",
      "Also, the problem is highly non-linear, meaning a linear classification methods (like linear regression, but for predicting discete categories) don't perform so well on the raw data.\n",
      "The MNIST [website](http://yann.lecun.com/exdb/mnist/) reports an extensive list of results obtained by different machine learning models, including neural nets, SVM, nearest neighbors, and others.\n",
      "\n",
      "The data consists of 60,000 training images and 10,000 test images.\n",
      "Each image is a 28x28 pixel, grayscale picture of a digit written either by a highschool student or an employee of the US Census Bureau.\n",
      "The images have all been preprocessed to be clean and regular: only one digit appears in each image, and it appears directly in the center of the image.\n",
      "\n",
      "The goal of the benchmark is to fit a model to the training set, and then use that model to predict which digit is in each of the test images.\n",
      "The best results achieve a classification error rate of less than half of one percent.\n",
      "This is often described as the \"human error rate,\" because if you ask people to classify the images, they will also find about 0.5% of them to be comepletely inscutable."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### MNIST Data\n",
      "\n",
      "Let's take a look at the data.\n",
      "MNIST data is easy to load with the built in scikit learn `datasets` module.\n",
      "Make sure you've loaded the data set by running the code at the top of the lab.\n",
      "\n",
      "The image data itself is in `mnist.data`, and it's stored in a numpy [n-dimensional array](http://docs.scipy.org/doc/numpy/reference/arrays.ndarray.html) (n=2 in this case).\n",
      "Numpy arrays are a vector/matrix data structure that provide high performance numerical computing.\n",
      "\n",
      "We can find out the dimensions of the array with its `shape` property:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mnist.data.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "(70000, 784)"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are 70,000 rows and 784 columns.\n",
      "So each row is an image (60,0000 training images plus 10,000 test ones gives 70,000 total), and each column is a pixel value (784 = 28 * 28).\n",
      "\n",
      "Like Pandas DataFrames, numpy arrays give us a simple interface to summary statistics and subsets of the data.\n",
      "\n",
      "Numpy arrays are indexed dimension-wise, with each dimension separated by a column:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "row = mnist.data[0,:] # First row of the array\n",
      "col = mnist.data[:,0] # First column of the array\n",
      "\n",
      "print row.shape\n",
      "print col.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(784,)\n",
        "(70000,)\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this syntax, the \":\" means \"ALL\", as in standard python indexing.\n",
      "All of the usual python range indexing syntax works for each dimension of the array.\n",
      "We can compute summary statistics, too:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print row.sum(), row.max(), row.min()\n",
      "print col.sum(), col.max(), col.min()\n",
      "\n",
      "print mnist.data[:10,:] # First ten rows\n",
      "print mnist.data[:,-10:] # Last ten columns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "31095 255 0\n",
        "0 0 0\n",
        "[[0 0 0 ..., 0 0 0]\n",
        " [0 0 0 ..., 0 0 0]\n",
        " [0 0 0 ..., 0 0 0]\n",
        " ..., \n",
        " [0 0 0 ..., 0 0 0]\n",
        " [0 0 0 ..., 0 0 0]\n",
        " [0 0 0 ..., 0 0 0]]\n",
        "[[0 0 0 ..., 0 0 0]\n",
        " [0 0 0 ..., 0 0 0]\n",
        " [0 0 0 ..., 0 0 0]\n",
        " ..., \n",
        " [0 0 0 ..., 0 0 0]\n",
        " [0 0 0 ..., 0 0 0]\n",
        " [0 0 0 ..., 0 0 0]]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Let's divide the array into two sets, one for training images and one for test:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train = mnist.data[:60000]\n",
      "test = mnist.data[60000:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that we can drop the trailing \",:\" when we want to just index the first dimension.\n",
      "\n",
      "### DIY\n",
      "\n",
      "* To start, we want to work with just a sample of the training data. Create a sample consisting of every 100th image in `test`.\n",
      "\n",
      "* Find the mean value of the 300th column in the sample data set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_sample = test[::100]\n",
      "\n",
      "test_sample[:300].mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "34.398954081632652"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Visualizing Image Data\n",
      "\n",
      "One of the nicest things about image data is that it is naturally visualized and understood.\n",
      "First, let's take a look at the raw data in the first image in the data set:\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "img = mnist.data[0]\n",
      "print img"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "   0  51 159 253 159  50   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "   0   0   0   0   0   0   0   0   0   0  48 238 252 252 252 237   0   0\n",
        "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "   0  54 227 253 252 239 233 252  57   6   0   0   0   0   0   0   0   0\n",
        "   0   0   0   0   0   0   0   0   0  10  60 224 252 253 252 202  84 252\n",
        " 253 122   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "   0 163 252 252 252 253 252 252  96 189 253 167   0   0   0   0   0   0\n",
        "   0   0   0   0   0   0   0   0   0   0  51 238 253 253 190 114 253 228\n",
        "  47  79 255 168   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "   0  48 238 252 252 179  12  75 121  21   0   0 253 243  50   0   0   0\n",
        "   0   0   0   0   0   0   0   0   0   0  38 165 253 233 208  84   0   0\n",
        "   0   0   0   0 253 252 165   0   0   0   0   0   0   0   0   0   0   0\n",
        "   0   7 178 252 240  71  19  28   0   0   0   0   0   0 253 252 195   0\n",
        "   0   0   0   0   0   0   0   0   0   0   0  57 252 252  63   0   0   0\n",
        "   0   0   0   0   0   0 253 252 195   0   0   0   0   0   0   0   0   0\n",
        "   0   0   0 198 253 190   0   0   0   0   0   0   0   0   0   0 255 253\n",
        " 196   0   0   0   0   0   0   0   0   0   0   0  76 246 252 112   0   0\n",
        "   0   0   0   0   0   0   0   0 253 252 148   0   0   0   0   0   0   0\n",
        "   0   0   0   0  85 252 230  25   0   0   0   0   0   0   0   0   7 135\n",
        " 253 186  12   0   0   0   0   0   0   0   0   0   0   0  85 252 223   0\n",
        "   0   0   0   0   0   0   0   7 131 252 225  71   0   0   0   0   0   0\n",
        "   0   0   0   0   0   0  85 252 145   0   0   0   0   0   0   0  48 165\n",
        " 252 173   0   0   0   0   0   0   0   0   0   0   0   0   0   0  86 253\n",
        " 225   0   0   0   0   0   0 114 238 253 162   0   0   0   0   0   0   0\n",
        "   0   0   0   0   0   0   0   0  85 252 249 146  48  29  85 178 225 253\n",
        " 223 167  56   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "  85 252 252 252 229 215 252 252 252 196 130   0   0   0   0   0   0   0\n",
        "   0   0   0   0   0   0   0   0   0   0  28 199 252 252 253 252 252 233\n",
        " 145   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "   0   0   0  25 128 252 253 252 141  37   0   0   0   0   0   0   0   0\n",
        "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "   0   0   0   0   0   0   0   0   0   0]\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These are all the pixel values in the image.\n",
      "We can see some patterns (e.g., the edges are empty), but it's hard to interpret.\n",
      "In fact, we can can a much better view if we use the `imshow` method from matplotlib to display an image:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pylab.imshow(img, cmap=\"Greys\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "Invalid dimensions for image data",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-8-9ec6d7802395>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpylab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Greys\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/usr/lib/pymodules/python2.7/matplotlib/pyplot.pyc\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, hold, **kwargs)\u001b[0m\n\u001b[0;32m   2375\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2376\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2377\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maspect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilternorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2378\u001b[0m         \u001b[0mdraw_if_interactive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2379\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib/pymodules/python2.7/matplotlib/axes.pyc\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   6794\u001b[0m                        filterrad=filterrad, resample=resample, **kwargs)\n\u001b[0;32m   6795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6796\u001b[1;33m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6797\u001b[0m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6798\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_artist_props\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib/pymodules/python2.7/matplotlib/image.pyc\u001b[0m in \u001b[0;36mset_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    409\u001b[0m         if (self._A.ndim not in (2, 3) or\n\u001b[0;32m    410\u001b[0m             (self._A.ndim == 3 and self._A.shape[-1] not in (3, 4))):\n\u001b[1;32m--> 411\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid dimensions for image data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_imcache\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mTypeError\u001b[0m: Invalid dimensions for image data"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAQcAAAD9CAYAAACx1bJsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYCIOAEzEAYCJOAAw/R98mB0LxqrbzAAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0xa3607ec>"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Annnnnd... it breaks.\n",
      "What went wrong?\n",
      "Since images are two dimensional objects, `imshow` expects a two dimensional array of data to plot, but the rows of our data array are flat vectors.\n",
      "Luckily, numpy arrays provide a `reshape` method that let's us change the dimensions of our data, (as long as we leave total length the same).\n",
      "Let's reshape our image data into a 28x28 pixel square and try again:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pylab.imshow(img.reshape(28, 28), cmap=\"Greys\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It's a zero!\n",
      "Now we're getting somewhere.\n",
      "\n",
      "### DIY\n",
      "\n",
      "* Use `imshow` to visualize a number of images from `sample`.  What can you say about how the data set is ordered?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#grouped by digit and ascends\n",
      "pylab.imshow(mnist.data[10000].reshape(28, 28), cmap=\"Greys\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Unsupervised KNN\n",
      "\n",
      "Next, we're interested in uncovering more structure in the MNIST data.\n",
      "For example, we want to be able to answer questions like \"how similar are people's handwriting?\" and \"how distinct are the different digits?\"\n",
      "If we get a sense of the variance in the data, and of how tighly it is clustered, we can begin to see a good approach to modeling.\n",
      "\n",
      "In the spirit of doing the simplest possible thing that might work, we can look at nearest neighbors using simple Euclidean distance between pixels.\n",
      "The assumption is that most digits look the same, so they should have similar values in individual pixels.\n",
      "Let's find out if this assumption is a good one.\n",
      "\n",
      "We can use the [`sklearn.neighbors`](http://scikit-learn.org/stable/modules/neighbors.html) module to compute nearest neighbors, in particular with the [`NearestNeighbors`](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors) class:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "from sklearn.neighbors import NearestNeighbors\n",
      "model = NearestNeighbors(algorithm='brute').fit(train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note how fast we built a nearest neighbors model, just a few microseconds!\n",
      "This is because we're using the brute force implementation (`algorithm='brute'`), which simple stores the training data to build a model, and does a full pairwise comparison at query time.\n",
      "\n",
      "Let's query our new model.  We can fetch the *k* nearest neighbors with the `kneighbors` method:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "query_img = test[0]\n",
      "_, result = model.kneighbors(query_img, n_neighbors=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that query time is significant, even for a single image.\n",
      "Also, notice that `kneighbors` returns two values.\n",
      "The first, which we will ignore, are the distance values to the nearest neighbors.\n",
      "The second is a list of indices where we can look up the nearest neighbors in the training set.\n",
      "\n",
      "With the results, now we can see how we did."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are four results, as expected.\n",
      "Let's print them out with the utility function below:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Display several images in a row\n",
      "def show(imgs, n=1):\n",
      "    fig = pylab.figure()\n",
      "    for i in xrange(0, n):\n",
      "        fig.add_subplot(1, n, i, xticklabels=[], yticklabels=[])\n",
      "        if n == 1:\n",
      "            img = imgs\n",
      "        else:\n",
      "            img = imgs[i]\n",
      "        pylab.imshow(img.reshape(28, 28), cmap=\"Greys\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "show(query_img)\n",
      "show(train[result[0],:], len(result[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The neighbors look pretty good!\n",
      "Importantly, they are all zeros.\n",
      "That means that to some extent, at least, our assumption about images of the same digit being \"close\" to one another in pixel-space is a good one.\n",
      "\n",
      "### DIY\n",
      "\n",
      "* Use the nearest neighbors model to inspect results for other images in the test set.  Do all of the digits seem to perform as well as \"0\" does?  You may want to increase \"k\" to find where things break down."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## KNN Classification\n",
      "\n",
      "We can validate our model in a more rigorous way by using it to predict digits.\n",
      "Scikit learn provides a class for supervised nearest neighbors fitting called [`KNeighborsClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier).\n",
      "It is very similar to the `NearestNeighbors` class, but it accepts labels when fitting a model, and it provides methods for making label predictions for test data.\n",
      "\n",
      "The MNIST labels are in `mnist.target`.\n",
      "Let's split them into training and test sets as we did with the image data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_labels = mnist.target[:60000]\n",
      "test_labels = mnist.target[60000:]\n",
      "test_labels_sample = test_labels[::100]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, as before, we fit a model to the training data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "from sklearn.neighbors import KNeighborsClassifier\n",
      "model = KNeighborsClassifier(n_neighbors=4, algorithm='brute').fit(train, train_labels)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 12 ms, sys: 8 ms, total: 20 ms\n",
        "Wall time: 70 ms\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## DIY\n",
      "\n",
      "* Use the KNeighborsClassifier.score method to measure the model's classification accuracy on `test_sample`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "model.score(test_sample, test_labels_sample)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 7.71 s, sys: 816 ms, total: 8.52 s\n",
        "Wall time: 12.3 s\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "0.96999999999999997"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Next, visualize the nearest neighbors of cases where the model makes erroneous predictions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "preds = model.predict(test_sample)\n",
      "errors = [i for i in xrange(0, len(test_sample)) if preds[i] != test_labels_sample[i]]\n",
      "          \n",
      "for i in errors:\n",
      "    query_img = test_sample[i]\n",
      "    _, result = model.kneighbors(query_img, n_neighbors=4)\n",
      "    show(query_img)\n",
      "    #show(test_sample[result[i],:], len(result[i]))\n",
      "    #pass # Visualize error image and its nearest neighbors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-12-eb2669ec6eec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mquery_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_sample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;31m#show(test_sample[result[i],:], len(result[i]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#pass # Visualize error image and its nearest neighbors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib/pymodules/python2.7/matplotlib/pyplot.pyc\u001b[0m in \u001b[0;36mshow\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \"\"\"\n\u001b[0;32m    138\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0m_show\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m     \u001b[0m_show\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/IPython/kernel/zmq/pylab/backend_inline.pyc\u001b[0m in \u001b[0;36mshow\u001b[1;34m(close)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mshow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_to_draw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m             \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'all'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Finally, let's get a more global view of classification errors.  To do this, we can create a **confusion matix**, that counts how often each class is mistaken for each other class.  Use [`sklearn.metric.confusion_matrix`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) to generate a confusion matrix between model predictions and test labels.  Which pair of digits are confused most frequently?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_sample = test[::20]\n",
      "test_labels_sample = test_labels[::20]\n",
      "\n",
      "def plot_cm(cm):\n",
      "    pylab.matshow(np.log(cm))\n",
      "   \n",
      "    \n",
      "from sklearn.metrics import confusion_matrix\n",
      "# Compute and plot the confusion matrix for test_sample\n",
      "plot_cm(confusion_matrix(test_labels_sample,model.predict(test_sample)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-17-6b0acc44aea1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Compute and plot the confusion matrix for test_sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mplot_cm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_sample\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_sample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/neighbors/classification.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0matleast2d_or_csr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0mneigh_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/neighbors/base.pyc\u001b[0m in \u001b[0;36mkneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    292\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meffective_metric_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'euclidean'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m                 dist = pairwise_distances(X, self._fit_X, 'euclidean',\n\u001b[1;32m--> 294\u001b[1;33m                                           squared=True)\n\u001b[0m\u001b[0;32m    295\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m                 dist = pairwise_distances(X, self._fit_X,\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/metrics/pairwise.pyc\u001b[0m in \u001b[0;36mpairwise_distances\u001b[1;34m(X, Y, metric, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m    655\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPAIRWISE_DISTANCE_FUNCTIONS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    656\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 657\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    658\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/metrics/pairwise.pyc\u001b[0m in \u001b[0;36meuclidean_distances\u001b[1;34m(X, Y, Y_norm_squared, squared)\u001b[0m\n\u001b[0;32m    174\u001b[0m                 \"Incompatible dimensions for Y and Y_norm_squared\")\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m     \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[0mdistances\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/sklearn/utils/extmath.pyc\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}