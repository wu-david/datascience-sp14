{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "CS194-16 Introduction to Data Science\n",
      "\n",
      "**Name**: David Wu\n",
      "\n",
      "**Student ID**: 22934420\n",
      "\n",
      "\n",
      "Assignment 1: Text Analysis and Entity Resolution\n",
      "===\n",
      "\n",
      "## Overview\n",
      "\n",
      "Entity resolution is a common, yet difficult problem in data cleaning and integration.\n",
      "In this assignment, we will use powerful and scalable text analysis techniques to perform entity resolution across two data sets of commercial products."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Entity Resolution\n",
      "\n",
      "Entity resolution, also known as record deduplication, is the process of identifying rows in one or more data sets that refer to the same real world entity.\n",
      "Take an example.\n",
      "You're on ebay looking for a [hip data science accessory][sliderulewiki], but you're on a budget, so you decide to scrape the [ebay listings][ebay] for a few days to get a feel for the market.\n",
      "Unfortunately, the listings are confusing and you don't know how to aggregate them.\n",
      "Entity resolution to the rescue!\n",
      "You find an [authoritative database][sliderules] and map all the ebay listings to it.\n",
      "Now you can comparison shop, get that sweet Keuffel and Esser for way cheap, and impress all the data hipsters.\n",
      "\n",
      "But finding matching records is a hard problem in general.\n",
      "A major reason is that the criteria for identifying duplicates are often vague and impossible to encode in rules.\n",
      "In addition, from a purely computational perspective, the problem is quadratic in the size of its inputs: naively, all pairs of records need to be compared to find all the duplicates.\n",
      "In this assignment, we will begin to address both these challenges.\n",
      "\n",
      "[sliderulewiki]: http://en.wikipedia.org/wiki/Slide_rule \"Only 2X faster than Hadoop\"\n",
      "\n",
      "[sliderules]: http://www.sliderule.ca/ \"Artisanal Data Technology\"\n",
      "\n",
      "[ebay]: http://www.ebay.com/sch/i.html?_odkw=keuffel+esser+slide+rule&_osacat=0&_trksid=p2045573.m570.l1313.TR0.TRC0.Xslide+rule&_nkw=slide+rule&_sacat=0&_from=R40 \"Impress your colleagues\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "### Application\n",
      "\n",
      "Your assignment is to perform entity resolution over two web-scraped data sets of commercial product listings, one from Amazon, and one from Google.\n",
      "The goal is to build a unified database of all products listed on the Internet: a one-stop-shop for all your shopping needs.  (Elevator pitch: *it's like **Kayak.com** for **e-commerce**!*)\n",
      "\n",
      "The web has made unprecedented amounts of data available publicly, but scraped data frequently needs to be de-duplicated.\n",
      "These data sets are typical examples of what you can collect with some simple scripting.\n",
      "The data is not especially large (just a few thousand records), but even so, you will find that entity resolution is a major challenge (top results with this data are ~50% success rate).\n",
      "Don't get discouraged; the goal is to get acquainted with techniques to tackle the problem, and apply them to a representative example.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Files\n",
      "\n",
      "Data files for this assignment can be found at:\n",
      "\n",
      "`https://github.com/amplab/datascience-sp14/raw/master/hw1/hw1data.tar.gz`\n",
      "\n",
      "The zip file includes the following files:\n",
      "\n",
      "* **Google.csv**, the Google Products data set \n",
      "* **Amazon.csv**, the Amazon data set\n",
      "* **Google_small.csv**, 200 records sampled from the Google data\n",
      "* **Amazon_small.csv**, 200 records sampled from the Amazon data\n",
      "* **Amazon_Google_perfectMapping.csv**, the \"gold standard\" mapping\n",
      "* **stopwords.txt**, a list of common English words\n",
      "\n",
      "Besides the complete data files, there are \"sample\" data files for each data set.\n",
      "Use these for **Part 1**.\n",
      "In addition, there is a \"gold standard\" file that contains all of the true mappings between entities in the two data sets.\n",
      "Every row in the gold standard file has a pair of record IDs (one Google, one Amazon) that belong to two record that describe the same thing in the real world.\n",
      "We will use the gold standard to evaluate our algorithms."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Deliverables\n",
      "\n",
      "Complete the all the exercises below and turn in a write up in the form of an IPython notebook, that is, **an .ipynb file**.\n",
      "The write up should include your code, answers to exercise questions, and plots of results.\n",
      "Complete submission instructions will be posted on Piazza.\n",
      "\n",
      "You can use this notebook and fill in answers inline, or if you prefer, do your write up in a separate notebook.\n",
      "In this notebook, we provide code templates for many of the exercises.\n",
      "They are intended to help with code re-use, since the exercises build on each other, and are highly recommended.\n",
      "Don't forget to include answers to questions that ask for natural language responses, i.e., in English, not code!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Guidelines\n",
      "\n",
      "#### Code\n",
      "\n",
      "This assignment can be done with basic python and matplotlib.\n",
      "Feel free to use PANDAs, too, which you may find well suited to several exercises.\n",
      "As for other libraries, please check with course staff whether they're allowed.\n",
      "In general, we want you to use whatever is comfortable, except for libraries (e.g., NLTK) that include functionality covered in the assignment.\n",
      "\n",
      "You're not required to do your coding in IPython, so feel free to use your favorite editor or IDE.\n",
      "But when you're done, remember to put your code into a notebook for your write up.\n",
      "\n",
      "#### Collaboration\n",
      "\n",
      "This assignment is to be done individually.  Everyone should be getting a hands on experience in this course.  You are free to discuss course material with fellow students, and we encourage you to use Internet resources to aid your understanding, but the work you turn in, including all code and answers, must be your own work."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 0: Preliminaries\n",
      "\n",
      "### Exercise 0\n",
      "\n",
      "Download the data and unzip it.\n",
      "Read each file in from the file system, and store them as lists of lines.\n",
      "\n",
      "For each of the data files (\"Google.csv\", \"Amazon.csv\", and the samples), we want to parse the IDs out of each record.\n",
      "The IDs are the first column of the file (they are URLs for Google, and alphanumeric strings for Amazon).\n",
      "Omitting the headers, load these data files into *dictionaries mapping ID to a string containing the rest of the record*.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "DATA_PATH = \"/home/saasbook/datascience-sp14/hw1/data/\"\n",
      "import csv\n",
      "\n",
      "amazon = {}\n",
      "amazon_file = csv.DictReader(open(DATA_PATH + \"Amazon.csv\"))\n",
      "for row in amazon_file:\n",
      "    amazon[row['id']] = row[\"title\"] +' ' + row[\"description\"] +' ' + row[\"manufacturer\"] +' ' + row[\"price\"]\n",
      "\n",
      "amazon_small = {}\n",
      "amazon_file = csv.DictReader(open(DATA_PATH + \"Amazon_small.csv\"))\n",
      "for row in amazon_file:\n",
      "    amazon_small[row['id']] = row[\"title\"] + ' ' +row[\"description\"] +' ' + row[\"manufacturer\"] +' ' + row[\"price\"]\n",
      "\n",
      "google = {}\n",
      "google_file = csv.DictReader(open(DATA_PATH + \"Google.csv\"))\n",
      "for row in google_file:\n",
      "    google[row['id']] = row[\"name\"] +' ' + row[\"description\"] +' ' + row[\"manufacturer\"] + ' ' +row[\"price\"]\n",
      "\n",
      "google_small = {}\n",
      "google_file = csv.DictReader(open(DATA_PATH + \"Google_small.csv\"))\n",
      "for row in google_file:\n",
      "    google_small[row['id']] = row[\"name\"] + ' ' +row[\"description\"] + ' ' +row[\"manufacturer\"] + ' ' +row[\"price\"]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 1: ER as Text Similarity\n",
      "\n",
      "A simple approach to entity resolution is to treat all records as strings and compute their similarity with a string distance function.\n",
      "In this section, we will build some components for bag-of-words text-analysis, and use them to compute record similarity.\n",
      "\n",
      "### 1.1 Bags of Words\n",
      "\n",
      "Bag-of-words is a conceptually simple yet powerful approach to text analysis.\n",
      "The idea is to treat strings, a.k.a. **documents**, as *unordered collections* of words, or **tokens**, i.e., as bags of words.\n",
      "\n",
      "> **Note on terminology**: \"token\" is more general than what we ordinarily mean by \"word\" and includes things like numbers, acronyms, and other exotica like word-roots and fixed-length character strings.\n",
      "> Bag of words techniques all apply to any sort of token, so when we say \"bag-of-words\" we really mean \"bag-of-tokens,\" strictly speaking.\n",
      "\n",
      "Tokens become the atomic unit of text comparison.\n",
      "If we want to compare two documents, we count how many tokens they share in common.\n",
      "If we want to search for documents with keyword queries (this is what Google does), then we turn the keywords into tokens and find documents that contain them.\n",
      "\n",
      "The power of this approach is that it makes string comparisons insensitive to small differences that probably do not affect meaning much, for example, punctuation and word order.\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Exercise 1\n",
      "\n",
      "**a**. Implement the function `simple_tokenize(string)` that takes a string and returns a list of tokens in the string.\n",
      "`simple_tokenize` should split strings using the provided regular expression.\n",
      "Since we want to make token-matching case insensitive, make sure all tokens are lower-case.\n",
      "Give an interpretation, in natural language, of what the regular expression, `split_regex`, matches."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "\n",
      "quickbrownfox = \"A quick brown fox jumps over the lazy dog.\"\n",
      "split_regex = r'\\W+'\n",
      "\n",
      "def simple_tokenize(string):\n",
      "    return filter(None, re.split(split_regex, string.lower()))\n",
      "\n",
      "print simple_tokenize(quickbrownfox) # Should give ['a', 'quick', 'brown', ... ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['a', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> TODO Answer questions\n",
      "\n",
      "> split_regex specifies a set of strings that match it.  The prefix \"r\" is used to indicate a raw string. '\\W+' indicates it'll look for non-alphanumeric characters. The + indicates one or more repetitions. In this case, when split_regex is used it will look for any non-alphanumeric character as a delimiter between words to split."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**b**. *Stopwords* are common words that do not contribute much to the content or meaning of a document (e.g., \"the\", \"a\", \"is\", \"to\", etc.).\n",
      "Stopwords add noise to bag-of-words comparisons, so the are usually excluded.\n",
      "Using the included file \"stopwords.txt\", implement `tokenize`, an improved tokenizer that does not emit stopwords.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stopwords = [line.rstrip() for line in open(DATA_PATH + \"stopwords.txt\", 'r')] # Load from file\n",
      "\n",
      "# TODO Implement this\n",
      "def tokenize(string):\n",
      "    return [word for word in simple_tokenize(string) if word not in stopwords]\n",
      "\n",
      "print tokenize(quickbrownfox) # Should give ['quick', 'brown', ... ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**c**. Now let's tokenize the two small data sets.\n",
      "For each one build a dictionary of tokens, i.e., a dictionary where the record IDs are the keys, and the output of `tokenize` is the values.\n",
      "How many tokens, total, are there in the two data sets?\n",
      "Which Amazon record has the biggest number of tokens?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO Compute these (dict() or DataFrame OK)\n",
      "amazon_rec2tok = {}\n",
      "google_rec2tok = {}\n",
      "total_tokens = 0\n",
      "\n",
      "for key in amazon_small:\n",
      "    tokens = tokenize(amazon_small[key])\n",
      "    amazon_rec2tok[key] = tokens\n",
      "    total_tokens += len(tokens)\n",
      "for key in google_small:\n",
      "    tokens = tokenize(google_small[key])\n",
      "    google_rec2tok[key] = tokens\n",
      "    total_tokens += len(tokens)\n",
      "\n",
      "print 'There are %s tokens in the combined data sets' % total_tokens\n",
      "\n",
      "biggest_record = max(amazon_rec2tok, key = lambda x:len(amazon_rec2tok[x]))\n",
      "print 'The Amazon record with ID \"%s\" has the most tokens' % biggest_record"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 23242 tokens in the combined data sets\n",
        "The Amazon record with ID \"b000o24l3q\" has the most tokens\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1.2 Weighted Bag-of-Words: TF-IDF\n",
      "\n",
      "Bag-of-words comparisons are not very good when all tokens are treated the same: some tokens are more important than others.\n",
      "Weights give us a way to specify which tokens to favor.\n",
      "With weights, when we compare documents, instead of counting common tokens, we sum up the weights of common tokens.\n",
      "\n",
      "A good heuristic for assigning weights is called \"Term-Frequency/Inverse-Document-Frequency,\" or TF-IDF for short.\n",
      "\n",
      "#### TF\n",
      "\n",
      "TF rewards tokens that appear many times in the same document.\n",
      "It is computed as the frequency of a token in a document, that is, if document `d` contains 100 tokens and token `t` appears in `d` 5 times, then the TF weight of `t` in `d` is `5/100 = 1/20`.\n",
      "The intuition for TF is that if a word occurs often in a document, then it is more important to the meaning of the document.\n",
      "\n",
      "#### IDF\n",
      "\n",
      "IDF rewards tokens that are rare overall in a data set.\n",
      "The intuition is that it is more significant if two documents share a rare word than a common one.\n",
      "IDF weight for a token, *t*, in a set of documents, *U*, is computed as follows: \n",
      "\n",
      "* Let *N* be the total number of documents in *U*\n",
      "* Find *n(t)*, the number of documents in *U* that contain *t*\n",
      "* Then *IDF(t) = N/n(t)*.\n",
      "\n",
      "Note that *n(t)/N* is the frequency of *t* in *U*, and *N/n* is the inverse frequency.\n",
      "\n",
      "> **Note on terminology**: Sometimes token weights depend on the document the token belongs to, that is, the same token may have a different weight when it's found in different documents.  We call these weights `local` weights.  TF is an example of a local weight, because it depends on the length of the source.  On the other hand, some token weights only depend on the token, and are the same everywhere that token is found.  We call these weights `global`, and IDF is one such weight.\n",
      "\n",
      "#### TF-IDF\n",
      "\n",
      "Finally, to bring it all together, the total TF-IDF weight for a token in a document is the product of its TF and IDF weights."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Exercise 2\n",
      "\n",
      "**a**. Implement `tf(tokens)` that takes a list of tokens belonging to a single document and returns a dictionary mapping tokens to TF weights.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO Implement this\n",
      "from collections import Counter\n",
      "def tf(tokens):\n",
      "    total = float(len(tokens))\n",
      "    count = Counter()\n",
      "    for word in tokens:\n",
      "        count[word] += 1\n",
      "    count = dict(count)\n",
      "    for word in count:\n",
      "        count[word] = float(count[word]) / total\n",
      "    return count\n",
      "    \n",
      "\n",
      "print tf(tokenize(quickbrownfox)) # Should give { 'quick': 0.1666 ... }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'brown': 0.16666666666666666, 'lazy': 0.16666666666666666, 'quick': 0.16666666666666666, 'jumps': 0.16666666666666666, 'fox': 0.16666666666666666, 'dog': 0.16666666666666666}\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**b**. Implement `find_idfs` that assigns an IDF weight to every unique token in a collection of data called `corpus`.  You may structure `corpus` however you want, but `find_idfs` should return a dictionary mapping tokens to weights.  Use `find_idfs` to compute IDF weights for all tokens in the combined small data sets.  How many unique tokens are there?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# corpus is a list of dictionaries. Each dictionary is a document\n",
      "def find_idfs(corpus):\n",
      "    idfs = Counter()\n",
      "    total = 0 # total documents\n",
      "    for d in corpus:\n",
      "        for key in d:\n",
      "            total += 1\n",
      "            words = set()\n",
      "            for word in tokenize(d[key]):\n",
      "                words.add(word)\n",
      "            for word in words:\n",
      "                idfs[word] += 1\n",
      "                \n",
      "    for key in idfs:\n",
      "        idfs[key] = float(total) / float(idfs[key])\n",
      "        \n",
      "    idfs = dict(idfs)\n",
      "    #print idfs\n",
      "    return idfs\n",
      "\n",
      "idfs_small = find_idfs([amazon_small, google_small]) # Use find_idfs here\n",
      "#print len(idfs_small)\n",
      "unique_tokens = len(idfs_small)\n",
      "#print idfs_small\n",
      "\n",
      "print \"There are %s unique tokens in the small data sets.\" % unique_tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 4900 unique tokens in the small data sets.\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**c**. What are the 10 tokens with the smallest IDF in the combined small data set?  Do you think they are useful for entity resolution?  Why or why not?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import operator\n",
      "small_idf_tokens = [val[0] for val in sorted(idfs_small.items(), key=operator.itemgetter(1))[0:10]]\n",
      "\n",
      "print small_idf_tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['99', '0', 'software', '9', 'new', 'features', 'use', '95', 'complete', 'easy']\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> TODO Answer questions\n",
      "\n",
      "> The ten tokens are: ['99', '0', 'software', '9', 'new', 'features', 'use', '95', 'complete', 'easy']\n",
      "\n",
      "> Small IDFs can still be useful for entity resolution. Since these words appear in many documents, it may indicate a type of language or keywords used to discuss certain topics; however, in most cases, these words probably need to be filtered out with stopwords. In this case, these tokens do not seem too useful for our entity resolution."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**d**. Plot a histogram of IDF values.  Be sure to use appropriate scaling and bucketing for the data.  What conclusions can you draw from the distribution of weights?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pylab\n",
      "%matplotlib inline\n",
      "\n",
      "pylab.hist(idfs_small.values(), bins=100)\n",
      "# TODO Make a plot. HINT: You can use pylab.hist"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "(array([   6,   23,   32,   29,   39,   54,   38,   36,   30,   34,   36,\n",
        "         66,    0,   76,    0,    0,  104,    0,    0,  153,    0,    0,\n",
        "          0,    0,  256,    0,    0,    0,    0,    0,    0,    0,  420,\n",
        "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "          0,    0,    0,    0,    0,  915,    0,    0,    0,    0,    0,\n",
        "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
        "       2553]),\n",
        " array([   2.98507463,    6.95522388,   10.92537313,   14.89552239,\n",
        "         18.86567164,   22.8358209 ,   26.80597015,   30.7761194 ,\n",
        "         34.74626866,   38.71641791,   42.68656716,   46.65671642,\n",
        "         50.62686567,   54.59701493,   58.56716418,   62.53731343,\n",
        "         66.50746269,   70.47761194,   74.44776119,   78.41791045,\n",
        "         82.3880597 ,   86.35820896,   90.32835821,   94.29850746,\n",
        "         98.26865672,  102.23880597,  106.20895522,  110.17910448,\n",
        "        114.14925373,  118.11940299,  122.08955224,  126.05970149,\n",
        "        130.02985075,  134.        ,  137.97014925,  141.94029851,\n",
        "        145.91044776,  149.88059701,  153.85074627,  157.82089552,\n",
        "        161.79104478,  165.76119403,  169.73134328,  173.70149254,\n",
        "        177.67164179,  181.64179104,  185.6119403 ,  189.58208955,\n",
        "        193.55223881,  197.52238806,  201.49253731,  205.46268657,\n",
        "        209.43283582,  213.40298507,  217.37313433,  221.34328358,\n",
        "        225.31343284,  229.28358209,  233.25373134,  237.2238806 ,\n",
        "        241.19402985,  245.1641791 ,  249.13432836,  253.10447761,\n",
        "        257.07462687,  261.04477612,  265.01492537,  268.98507463,\n",
        "        272.95522388,  276.92537313,  280.89552239,  284.86567164,\n",
        "        288.8358209 ,  292.80597015,  296.7761194 ,  300.74626866,\n",
        "        304.71641791,  308.68656716,  312.65671642,  316.62686567,\n",
        "        320.59701493,  324.56716418,  328.53731343,  332.50746269,\n",
        "        336.47761194,  340.44776119,  344.41791045,  348.3880597 ,\n",
        "        352.35820896,  356.32835821,  360.29850746,  364.26865672,\n",
        "        368.23880597,  372.20895522,  376.17910448,  380.14925373,\n",
        "        384.11940299,  388.08955224,  392.05970149,  396.02985075,  400.        ]),\n",
        " <a list of 100 Patch objects>)"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD9CAYAAACx+XApAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDEcQAIDhCAIAMBxBAACG+z/WQK6GHt4KTAAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0xa4cb72c>"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> TODO Answer questions\n",
      "\n",
      "> Most tokens only appear once in a document. The distribution also seems logirthmic such that many words only appear in one document, less in two, and even less in three and so forth."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**e**. Use `tf` to implement `tfidf(tokens, idfs)` that takes a list of tokens from a document and a dictionary of idf weights and returns a dictionary mapping tokens to total TF-IDF weight.  Use `tfidf` to compute the weights of Amazon product record 'b000hkgj8k'."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO Implement this\n",
      "def tfidf(tokens, idfs):\n",
      "    tf_weights = tf(tokens)\n",
      "    tf_idf = {}\n",
      "    for token in tokens:\n",
      "        tf_idf[token] = float(tf_weights[token]) * float(idfs[token])\n",
      "    return tf_idf\n",
      "\n",
      "rec_b000hkgj8k_weights = tfidf(tokenize(amazon_small['b000hkgj8k']), idfs_small)\n",
      "\n",
      "print \"Amazon record 'b000hkgj8k' has tokens and weights:\\n%s\" % rec_b000hkgj8k_weights"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Amazon record 'b000hkgj8k' has tokens and weights:\n",
        "{'autocad': 28.57142857142857, 'autodesk': 7.142857142857142, '47': 14.285714285714285, 'courseware': 57.14285714285714, 'psg': 28.57142857142857, '2007': 3.007518796992481, 'customizing': 14.285714285714285, 'interface': 2.5974025974025974, '95': 0.5194805194805194}\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "### 1.3 Cosine Similarity\n",
      "\n",
      "Now we are ready to do text comparisons in a formal way.\n",
      "The metric of string distance we will use is called **cosine similarity**.\n",
      "We will treat each document as a vector in some high dimensional space.\n",
      "Then, to compare two documents we compute the cosine of the angle between their two document vectors.\n",
      "This is easier than it sounds.\n",
      "\n",
      "The first question to answer is how do we represent documents as vectors?\n",
      "The answer is familiar: bag-of-words!\n",
      "We treat each unique token as a dimension, and treat token weights as magnitudes in their respective token dimensions.\n",
      "For example, suppose we use simple counts as weights, and we want to interpret the string \"Hello, world!  Goodbye, world!\" as a vector.\n",
      "Then in the \"hello\" and \"goodbye\" dimensions the vector has value 1, in the \"world\" dimension it has value 2, and it is zero in all other dimensions.\n",
      "\n",
      "Next question is: given two vectors how do we find the cosine of the angle between them?\n",
      "Recall the formula for the dot product of two vectors:\n",
      "\n",
      "$$a \\cdot b = \\| a \\| \\| b \\| \\cos \\theta$$\n",
      "\n",
      "Here $a \\cdot b = \\sum_{i=1}^n a_i b_i$ is the ordinary dot product of two vectors, and $\\|a\\| = \\sqrt{ \\sum_{i=1}^n a_i^2 }$ is the norm of $a$.\n",
      "\n",
      "We can rearrange terms and solve for the cosine to find it is simply the normalized dot product of the vectors.\n",
      "With our vector model, the dot product and norm computations are simple functions of the bag-of-words document representations, so we now have a formal way to compute similarity:\n",
      "\n",
      "$$similarity = \\cos \\theta = \\frac{a \\cdot b}{\\|a\\| \\|b\\|} = \\frac{\\sum_{i=1}^n a_i b_i}{\\sqrt{\\sum_{i=1}^n a_i^2} \\sqrt{\\sum_{i=1}^n b_i^2}}$$\n",
      "\n",
      "Setting aside the algebra, the geometric interpretation is more intuitive.\n",
      "The angle between two document vectors is small if they share many tokens in common, because they are pointing in roughly the same direction.\n",
      "Then, the cosine of the angle will be large.\n",
      "Otherwise, if the angle is large (and they have few words in common), the cosine is small.\n",
      "So the cosine scales proportionally with our intuitive sense of similarity."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "#### Exercise 3\n",
      "\n",
      "**a**. Implement `cosine_similarity(string1, string2, idfs)` that takes two strings and computes their cosine similarity in the context of some global IDF weights.\n",
      "Use `tokenize`, `tfidf`, and the IDF weights from exercise **2b** for extracting tokens and assigning them weights.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "\n",
      "# Optional utility\n",
      "def dotprod(a, b):\n",
      "    keys_a = set(a.keys())\n",
      "    keys_b = set(b.keys())\n",
      "    keys = keys_a & keys_b\n",
      "    total = 0\n",
      "    for key in keys:\n",
      "        total += a[key]*b[key]\n",
      "    return total\n",
      "\n",
      "# Optional utility\n",
      "def norm(a):\n",
      "    total = 0\n",
      "    for val in a.values():\n",
      "        total += val**2\n",
      "    return math.sqrt(total)\n",
      "\n",
      "# Optional freebie\n",
      "def cossim(a, b):\n",
      "    return dotprod(a, b) / norm(a) / norm(b)\n",
      "\n",
      "test_vec1 = {'foo': 2, 'bar': 3, 'baz': 5 }\n",
      "test_vec2 = {'foo': 1, 'bar': 0, 'baz': 20 }\n",
      "print dotprod(test_vec1, test_vec2), norm(test_vec1) # Should be 102 6.16441400297\n",
      "\n",
      "# TODO Implement this\n",
      "def cosine_similarity(string1, string2, idfs):\n",
      "    a = tfidf(tokenize(string1), idfs)\n",
      "    b = tfidf(tokenize(string2), idfs)\n",
      "    return cossim(a, b)\n",
      "\n",
      "print cosine_similarity(\"Adobe Photoshop\",\n",
      "                        \"Adobe Illustrator\", \n",
      "                        idfs_small) # Should be 0.0577243382163"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "102 6.16441400297\n",
        "0.0577243382163\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**b**. Now we can finally do some entity resolution!\n",
      "For every product record in the small Google data set, use `cosine_similarity` to compute its similarity to every record in the small Amazon set.  Build a dictionary mapping `(Amazon Id, Google Id)` tuples to similarity scores between 0 and 1.\n",
      "What is the similarity between Amazon record 'b000o24l3q' and Google record `http://www.google.com/base/feeds/snippets/17242822440574356561`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO Compute similarities\n",
      "similarities = {}\n",
      "for key_a in amazon_small:\n",
      "    for key_b in google_small:\n",
      "        similarities[(key_a, key_b)] = cosine_similarity(amazon_small[key_a], google_small[key_b], idfs_small)\n",
      "#print max(similarities.values())\n",
      "print 'Requested similarity is %s.' % similarities[('b000o24l3q',\n",
      "  'http://www.google.com/base/feeds/snippets/17242822440574356561')]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Requested similarity is 0.000301935456928.\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**c**.  Use the \"gold standard\" data (loaded from the included file) to answer the following questions.  How many true duplicate pairs are there in the small data set?  What is the average similarity score for true duplicates?  What about for non-duplicates?  Based on this, is cosine similarity doing a good job, qualitatively speaking, of identifying duplicates?  Why or why not?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import collections\n",
      "perfect_file = csv.DictReader(open(DATA_PATH + \"Amazon_Google_perfectMapping.csv\"))\n",
      "gold_standard = [(row['idAmazon'], row['idGoogleBase']) for row in perfect_file]\n",
      "\n",
      "true_dups = 0\n",
      "avg_sim_dups = 0.0\n",
      "avg_sim_non = 0.0\n",
      "dups = 0.0\n",
      "non_dups = 0.0\n",
      "\n",
      "for pair in similarities:  \n",
      "    if pair in gold_standard:\n",
      "        avg_sim_dups += similarities[pair]\n",
      "        dups += 1.0\n",
      "        true_dups += 1\n",
      "    else:\n",
      "        avg_sim_non += similarities[pair]\n",
      "        non_dups += 1.0\n",
      "        \n",
      "avg_sim_dups = avg_sim_dups / dups\n",
      "avg_sim_non = avg_sim_non /  non_dups\n",
      "\n",
      "print \"There are %s true duplicates.\" % true_dups\n",
      "print \"The average similarity of true duplicates is %s.\" % avg_sim_dups\n",
      "print \"And for non duplicates, it is %s.\" % avg_sim_non"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 146 true duplicates.\n",
        "The average similarity of true duplicates is 0.251111121887.\n",
        "And for non duplicates, it is 0.00116980178162.\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> TODO Answer questions\n",
      "\n",
      "> There are 146 true duplicates. The average similarity of true duplicates is 0.251111121887. The average similarity of non duplicates is 0.00116980178162. Based on these results, cosine similarity does a good job identifying duplicates as there is a significant difference between the average similarites of true duplicates and non duplicates"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 2: Scalable ER\n",
      "\n",
      "In the previous section we built a text similarity function and used it for small scale entity resolution.  Our implementation is limited by its quadratic run time complexity, and is not practical for even modestly sized data sets.  In this section we will implement a more scalable algorithm and use it to do entity resolution on the full data set.\n",
      "\n",
      "### Inverted Indices\n",
      "\n",
      "To improve our ER algorithm from **Part 1**, we should begin by analyzing its running time.\n",
      "In particular, the algorithm above is quadratic in two ways.\n",
      "First, we did a lot of redundant computation of tokens and weights, since each record was reprocessed every time it was compared.\n",
      "Second, we made qudratically many token comparisons between records.\n",
      "\n",
      "The first source of quadratic overhead can be eliminated with precomputation and look-up tables, but the second source is a little more tricky.\n",
      "In the worst case, every token in every record in one data set exists in every record in the other data set, and therefore every token makes a nonzero contribution to the cosine similarity.\n",
      "In this case, token comparison is unavoidably quadratic.\n",
      "\n",
      "But in reality most records have nothing (or very little) in common.\n",
      "Moreover, it is typical for a record in one data set to have at most one duplicate record in the other data set (this is the case assuming each data set has been de-duplicated against itself).\n",
      "In this case, the output is linear in the size of the input and we can hope to achieve linear running time.\n",
      "\n",
      "An **inverted index** is a data structure that will allow us to avoid making quadratically many token comparisons.  It maps each token in the data set to the list of documents that contain the token.  So, instead of comparing, record by record, each token to every other token to see if they match, we will use inverted indices to *look up* records that match on a particular token.\n",
      "\n",
      "> **Note on terminology**: In text search, a *forward* index maps documents in a data set to the tokens they contain.  An *inverted* index supports the inverse mapping.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Exercise 4\n",
      "\n",
      "> **Note**: For this section, use the complete Google and Amazon data sets, not the samples\n",
      "\n",
      "> **Pandas note**: If you use DataFrames for the mapping tables, make sure you index them correctly for efficient key look ups\n",
      "\n",
      "**a**. To address the overhead of recomputing tokens and token-weights, build a dictionary for each data set that maps record IDs to TF-IDF weighted token vectors (the vectors themselves should be dictionaries).  You will need to re-use code from above to recompute IDF weights for the complete combined data set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Redo tokenization for full data set\n",
      "amazon_rec2tok = {}\n",
      "google_rec2tok = {}\n",
      "\n",
      "for key in amazon:\n",
      "    tokens = tokenize(amazon[key])\n",
      "    amazon_rec2tok[key] = tokens\n",
      "for key in google:\n",
      "    tokens = tokenize(google[key])\n",
      "    google_rec2tok[key] = tokens\n",
      "\n",
      "# Recompute IDFs for full data set\n",
      "idfs = find_idfs([amazon, google])\n",
      "\n",
      "# Pre-compute TF-IDF weights.  Build mappings from record ID weight vector.\n",
      "google_weights = {}\n",
      "amazon_weights = {}\n",
      "\n",
      "# Pre-compute norms.  Build mappings from record ID to norm of the weight vector.\n",
      "google_norms = {}\n",
      "amazon_norms = {}\n",
      "for key in amazon_rec2tok:\n",
      "    amazon_weights[key] = tfidf(amazon_rec2tok[key], idfs)\n",
      "    amazon_norms[key] = norm(amazon_weights[key])\n",
      "\n",
      "for key in google_rec2tok:\n",
      "    google_weights[key] = tfidf(google_rec2tok[key], idfs)\n",
      "    google_norms[key] = norm(google_weights[key])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**b**. Build inverted indices of both data sources."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO Implement this. Should return a mapping from token to list-of-record-IDs\n",
      "def invert_index(forward_index):\n",
      "    inverse = { }\n",
      "    for key, tokens in forward_index.iteritems():\n",
      "        for token in tokens:\n",
      "            if token not in inverse:\n",
      "                inverse[token] = [key]\n",
      "            else:\n",
      "                inverse[token].append(key)\n",
      "    return inverse\n",
      "\n",
      "# Pre-compute inverted indices\n",
      "amazon_inv = invert_index(amazon_weights)\n",
      "google_inv = invert_index(google_weights)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**c**. We are now in position to efficiently perform ER on the full data sets.\n",
      "Implement the following algorithm to build a dictionary that maps a pair of records (as a tuple) to a list of tokens they share in common:\n",
      "Iterate over tokens of one data set, and for each token, if the token appears in the other data set, use the inverted indices to find all pairs of records (one from either set) that contain the token.  Add these pairs to the output."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import itertools\n",
      "\n",
      "# TODO Implement algorithm to compute this:        \n",
      "common_tokens = { } # Should map a record ID pair to a list of common tokens\n",
      "\n",
      "for token in amazon_inv:\n",
      "    a_ids = amazon_inv[token]\n",
      "    for rec_a in a_ids:\n",
      "        if token in google_inv:\n",
      "            g_ids = google_inv[token]\n",
      "            for rec_g in g_ids:\n",
      "                if (rec_a, rec_g) not in common_tokens:\n",
      "                    common_tokens[(rec_a, rec_g)] = [token]\n",
      "                else:\n",
      "                    common_tokens[(rec_a, rec_g)].append(token)\n",
      "        \n",
      "\n",
      "common_tokens = dict(common_tokens)\n",
      "\n",
      "print len(common_tokens) # Should be 2882143\n",
      "print common_tokens.keys()[0:20]\n",
      "print common_tokens.values()[0:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2882143\n",
        "[('b000egoe9s', 'http://www.google.com/base/feeds/snippets/13517735545841379480'), ('b0009suw5q', 'http://www.google.com/base/feeds/snippets/13573519961862572084'), ('b000egidpy', 'http://www.google.com/base/feeds/snippets/13693694360546677062'), ('b000h24200', 'http://www.google.com/base/feeds/snippets/17546591632033422235'), ('b000btas3y', 'http://www.google.com/base/feeds/snippets/17137990908475407260'), ('b000068u3h', 'http://www.google.com/base/feeds/snippets/1366247437960571613'), ('b000ndic40', 'http://www.google.com/base/feeds/snippets/10902155796053790562'), ('b000pgq8sm', 'http://www.google.com/base/feeds/snippets/13350752071839986401'), ('b000fetuiy', 'http://www.google.com/base/feeds/snippets/17787032142812456248'), ('b0001na35s', 'http://www.google.com/base/feeds/snippets/13369517855071639147'), ('b000lwfbem', 'http://www.google.com/base/feeds/snippets/17449789691934713419'), ('b00008mnxr', 'http://www.google.com/base/feeds/snippets/10346497411945900130'), ('b000abapo0', 'http://www.google.com/base/feeds/snippets/13635462580810024712'), ('b0002w30e4', 'http://www.google.com/base/feeds/snippets/17043638434818426828'), ('b000o231le', 'http://www.google.com/base/feeds/snippets/5068947095089747483'), ('b000icpfmi', 'http://www.google.com/base/feeds/snippets/10881355389329582885'), ('b000in8n30', 'http://www.google.com/base/feeds/snippets/13783139861924882499'), ('b000088ner', 'http://www.google.com/base/feeds/snippets/13106173681796275626'), ('b000bw7kxw', 'http://www.google.com/base/feeds/snippets/18375765231096501819'), ('b000053f92', 'http://www.google.com/base/feeds/snippets/13774824523880714097')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[['99', 'favorite', 'features', 'art', 'include'], ['99', 'video', '0', 'deluxe', 'create', 'copy'], ['v'], ['2007'], ['part'], ['system', 'macintosh', 'windows'], ['pdf', 'runs', 'available', 'x', '0', 'running', '6', 'higher'], ['easy', 'network', 'based', 'signature', 'integrated'], ['easy', 'use'], ['secure', 'security', 'maintenance', 'connectivity'], ['easy', 'complete', '0', 'easily', 'use'], ['5'], ['software'], ['99', 'way', 'mac'], ['software'], ['software'], ['software'], ['powerful'], ['single', 'user'], ['includes']]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**d**. Use the data structures from parts **a** and **c** to build a dictionary to map record pairs to cosine similarity scores."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO Implement this. Should take two record IDs and a list of common\n",
      "# tokens and return the cosine similarity of the two records.\n",
      "# Use results from part *a* for fast look ups.\n",
      "def fast_cosine_similarity(a_rec, g_rec, tokens):\n",
      "    total = 0.0\n",
      "    for token in tokens:\n",
      "        total += amazon_weights[a_rec][token] * google_weights[g_rec][token]\n",
      "    norm_a = amazon_norms[a_rec]\n",
      "    norm_b = google_norms[g_rec]\n",
      "    return total / norm_a / norm_b\n",
      "\n",
      "# Compute similarities (use fast_cosine_similarity)\n",
      "\n",
      "sims = {} # Should map record-ID-pairs to cosine similarity score\n",
      "\n",
      "for rec_pair in common_tokens.keys():\n",
      "    a = rec_pair[0]\n",
      "    g = rec_pair[1]\n",
      "    tokens = common_tokens[(a, g)]\n",
      "    sims[rec_pair] = fast_cosine_similarity(a, g, tokens)\n",
      "#print max(sims.values())\n",
      "#print sims.values()[0:50]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.00021384792692692153, 9.235874039453725e-05, 5.3139619223408655e-06, 2.3032729906585946e-05, 1.8654439244035683e-06, 1.259990565399865e-05, 2.1302158681044857e-05, 0.00023658776346616221, 0.0006902969017447497, 4.172970323640217e-06, 0.0008955822964484205, 3.900503392505612e-05, 1.314845141116225e-06, 0.0004698386487798861, 5.7686602769427614e-08, 2.3530492877158995e-05, 2.9521122461026157e-06, 2.1351650898688148e-07, 3.5491143288279235e-07, 4.089428218449306e-06, 6.185002853602851e-06, 8.387693783382046e-07, 6.554809723787993e-05, 4.681316672956473e-05, 5.8159420706072786e-05, 0.0006210128515043337, 0.00028209500806607575, 3.075328871552995e-06, 4.200314268855346e-06, 6.346091356164995e-05, 0.0002596204384333902, 9.500017546710918e-06, 4.475952380811344e-06, 0.0011214793964268407, 0.004345698620353578, 0.0004705559890294673, 4.66083913388044e-05, 4.2341266067075715e-06, 3.422825378928194e-06, 8.409123204099011e-06, 2.595965435380163e-06, 9.172474413537719e-06, 7.544872679418371e-06, 2.507525650002557e-07, 2.931836367316827e-07, 0.0007725725495481957, 2.4598401979267548e-05, 3.973407016325417e-06, 0.0010454671824720008, 3.0278455297421897e-06]\n"
       ]
      }
     ],
     "prompt_number": 50
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "### Analysis\n",
      "\n",
      "Now we have an authoritative list of record-pair similarities, but we need a way to use those similarities to decide if two records are duplicates or not.\n",
      "The simplest approach is to pick a **threshold**.\n",
      "Pairs whose similarity is above the threshold are declared duplicates, and pairs below the threshold are declared distinct.\n",
      "\n",
      "To decide where to set the threshold we need to understand what kind of errors result at different levels.\n",
      "If we set the threshold too low, we get more **false positives**, that is, record-pairs we say are duplicates that in reality are not.\n",
      "If we set the threshold too high, we get more **false negatives**, that is, record-pairs that really are duplicates but that we miss.\n",
      "\n",
      "ER algorithms are evaluated by the common metrics of information retrieval and search called **precision** and **recall**.\n",
      "Precision asks of all the record-pairs marked duplicates, what fraction are true duplicates?\n",
      "Recall asks of all the true duplicates in the data, what fraction did we successfully find?\n",
      "As with false positives and false negatives, there is a trade-off between precision and recall.\n",
      "A third metric, called **F-measure**, takes the harmonic mean of precision and recall to measure overall goodness in a single value.:\n",
      "\n",
      "$$Fmeasure = 2 \\frac{precision * recall}{precision + recall}$$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Exercise 5\n",
      "\n",
      "> **Note**: For this exercise, use the \"gold standard\" mapping from the included file to look up true duplicates, and the results of exercise 4.\n",
      "\n",
      "**a**. Implement functions to count true-positives (true duplicates above the threshold), and false-positives and -negatives.  HINT: To make your functions efficient, you should bin your counts by similarity range."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Look up all similarity scores for true duplicates\n",
      "keys = sims.keys()\n",
      "true_dup_sims = [] # TODO Build this\n",
      "for val in gold_standard:\n",
      "    if val in keys:\n",
      "        true_dup_sims.append(sims[(val[0], val[1])])\n",
      "    else:\n",
      "        true_dup_sims.append(0.0)\n",
      "#print max(true_dup_sims)\n",
      "#print len(true_dup_sims)\n",
      "# TODO Just compute true_dup_sim above\n",
      "def truepos(threshold):\n",
      "    return len(true_dup_sims) - falseneg(threshold)\n",
      "\n",
      "# Pre-bin counts of false positives by threshold range\n",
      "nthresholds = 100\n",
      "def bin(similarity):\n",
      "    return int(similarity * nthresholds)\n",
      "\n",
      "\n",
      "fp_counts = { } # TODO Build this.  Should map bin number to count of false-positives\n",
      "\n",
      "for key in keys:\n",
      "    similarity = sims[key]\n",
      "    if similarity not in true_dup_sims:\n",
      "        bucket = bin(similarity)\n",
      "        if bucket in fp_counts:\n",
      "            fp_counts[bucket] += 1\n",
      "        else:\n",
      "            fp_counts[bucket] = 1\n",
      "               \n",
      "print fp_counts\n",
      "\n",
      "            \n",
      "# TODO Implement this\n",
      "def falsepos(threshold):\n",
      "    threshold_bucket = bin(threshold)\n",
      "    total = 0\n",
      "    for bucket in fp_counts:\n",
      "        if bucket >= threshold_bucket:\n",
      "            total += fp_counts[bucket]\n",
      "    return total\n",
      "\n",
      "# TODO Implement this\n",
      "def falseneg(threshold):\n",
      "    threshold_bucket = bin(threshold)\n",
      "    negatives = 0\n",
      "    for sim in true_dup_sims:\n",
      "        true_pos = bin(sim)\n",
      "        if true_pos < threshold_bucket:\n",
      "            negatives += 1\n",
      "    return negatives\n",
      "\n",
      "print falseneg(0.95)\n",
      "print falsepos(0.95)\n",
      "print len(true_dup_sims)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{0: 2857981, 1: 11254, 2: 4003, 3: 2022, 4: 1229, 5: 788, 6: 577, 7: 423, 8: 360, 9: 262, 10: 208, 11: 179, 12: 163, 13: 133, 14: 124, 15: 85, 16: 74, 17: 73, 18: 53, 19: 67, 20: 63, 21: 54, 22: 49, 23: 39, 24: 26, 25: 28, 26: 17, 27: 26, 28: 24, 29: 28, 30: 27, 31: 21, 32: 20, 33: 17, 34: 16, 35: 12, 36: 12, 37: 13, 38: 14, 39: 16, 40: 14, 41: 15, 42: 13, 43: 8, 44: 11, 45: 11, 46: 14, 47: 11, 48: 12, 49: 5, 50: 8, 51: 3, 52: 4, 53: 3, 54: 3, 55: 1, 57: 9, 58: 4, 59: 4, 60: 8, 61: 2, 62: 1, 63: 1, 64: 6, 65: 3, 66: 7, 67: 3, 68: 3, 69: 6, 70: 3, 71: 4, 72: 6, 73: 1, 74: 2, 75: 1, 76: 4, 77: 1, 78: 2, 80: 2, 82: 2, 83: 2, 84: 1, 85: 3, 87: 1, 88: 2, 89: 1, 90: 1, 91: 1, 92: 1, 93: 2, 94: 3, 95: 1, 96: 2, 97: 2, 98: 3, 99: 4}\n",
        "1279\n",
        "12\n",
        "1300\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**b**. What is the relationship between false-positives and -negatives (and true-positives and -negatives) on the one hand and precision and recall on the other?\n",
      "Use the functions from part **a** to implement functions to compute precision, recall, F-measure as a function of threshold value."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TODO Implement this (returns a float)\n",
      "def precision(threshold):\n",
      "    true_pos = truepos(threshold)\n",
      "    return true_pos / float(falsepos(threshold) + true_pos)\n",
      "\n",
      "# TODO Implement this (returns a float)\n",
      "def recall(threshold):\n",
      "    return truepos(threshold) / float(len(true_dup_sims))\n",
      "\n",
      "# TODO Implement this (returns a float)\n",
      "def fmeasure(threshold):\n",
      "    p = precision(threshold)\n",
      "    r = recall(threshold)\n",
      "    return 2 * (( p * r ) /  ( p + r ))\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**c**. Make line plots of precision, recall, and F-measure as a function of threshold value, for thresholds between 0.0 and 1.0.  You can change `nthresholds` (above in part **a**) to change threshold values to plot."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# For the x-axis\n",
      "thresholds = [float(n) / nthresholds for n in range(0, nthresholds)]\n",
      "precisions = [precision(t) for t in thresholds]\n",
      "recalls = [recall(t) for t in thresholds]\n",
      "fmeasures = [fmeasure(t) for t in thresholds]\n",
      "\n",
      "pylab.plot(thresholds, precisions, label='Precision')\n",
      "pylab.plot(thresholds, recalls, label='Recall')\n",
      "pylab.plot(thresholds, fmeasures, label='Fmeasures')\n",
      "pylab.legend(loc='upper right')\n",
      "pylab.ylim(0.0, 1.0)\n",
      "pylab.xlim(0.0, 1.0)\n",
      "pylab.xlabel('Thresholds')\n",
      "pylab.ylabel('Ratios')\n",
      "pylab.title('Plots')\n",
      "\n",
      "\n",
      "\n",
      "# TODO Make a plot.  HINT: Use pylab.plot().  Don't forget labels."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "<matplotlib.text.Text at 0x1b24956c>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEVCAYAAADgh5I1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1b23342c>"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**d**. Using the plot, pick the optimal threshold value and argue for why it is optimal.\n",
      "If false-positives are considered much worse than false-negatives, how does that change your answer?\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> TODO Answer questions\n",
      "\n",
      "> 0.2 is optimal. It maximizes precision and recall as indicated by the maximization of Fmeasures. If false positives are considered much worse than false negatives, then the threshold should shift higher to compensate. As the threshold is shifted higher, we can expect more false negatives and less false positives"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**e**.  State-of-the-art tools can get an F-measure of about 60% on this data set.\n",
      "In this assignment we expect to get an F-measure closer to 40%.\n",
      "Look at some examples of errors (both false-positives and -negatives) and think about what went wrong.\n",
      "What are some ways we might improve our simple classifier?\n",
      "Back up your ideas with examples as much as possible."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> TODO Answer questions\n",
      "\n",
      "> To get a higher F-measure, we want to minimize the number of false-positives and false-negatives. One possible solution is to prune our simmilarities dictionary to not account for false positives. Since we are given which similarities are part of the true perfect mapping, we can prune out values that are needed. In practice, we would probably not be given these perfect matchings. In this case, the first step we could take is to remove more common words using the IDF weights. This would increase accuracy since documents that aren't truly matched would seem less like each other thereby decreasing false-positives. Furthermore, you could increase the weights on more rare words. Rather than linearly increasing the weights with regard to frequency, you could make the scale exponential. This has the same effect as those with more rare words would stand out more in the matchings. In this example one has tokens [kaplan sat/act/psat platinum 2007...] the other has tokens [kaplan act/sat/psat gold 2007..]. Using our domain knowledge that Gold and platinum is a different product, downweighting this pairs' score makes sense. The same logic can be applied to titles and years."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}